{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbIEjeP6-JVL"
   },
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuhe_XOaM7Wh"
   },
   "outputs": [],
   "source": [
    "!rm -rf ~/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRgmPo6ImKP9",
    "outputId": "f719d3ee-a39c-4ad2-d557-61c88f47cef3"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 openai scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXR-0j9KmdGr"
   },
   "outputs": [],
   "source": [
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcpBDppE-Sdz"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVDyTiJU6ZxO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FyYrKyn-X0b"
   },
   "source": [
    "#  QUANTIZATION CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TaP2FlJ9PXk"
   },
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBiByKFD-b80"
   },
   "source": [
    "# INITIALIZE LLAMA (MAIN MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401,
     "referenced_widgets": [
      "6bd422999f37430d83e51915bbcf10b3",
      "548c3fd1bfd84b8b81b0c90c97106db6",
      "90ee7a5edeaa43b1ba9c0aff100941a3",
      "8595c3f0dec545029775d75425031193",
      "8cfed8b591a849aab770b30baeb1761a",
      "d7271fef927f44c9af7218ab64961648",
      "25db165b1f084aae938fe709a5eadae4",
      "c648372c01594db7a8fcde55f527c7a9",
      "1d7aeb62cf3249c3b7cb4a0c1efd9343",
      "119be528082e469699e5ac5e7ace8521",
      "1661a24a38764d48b6c71b2a605d2e4f",
      "e6305b8f21824517bbb924362bfaa83c",
      "ff3cefea555f486eb4573f774df46312",
      "9bc34360e3e44b33b4084214610a102b",
      "f1511fee687b46c5b4adf04259fb2228",
      "09a4620026884b20b93fb95394015255",
      "eeb0e32b8e8b4297b4908e30766c1545",
      "fe70442965e64fcaa14b59d3ab3565c9",
      "43fb41e09d8548a593caa6c92b56d15b",
      "861557fdbf60494ba279039cc641c4b9",
      "78cc3024bf7d4bd4a1bfc457c80a07ad",
      "d10bfd4237a94d0fab09c837bb8b7b26",
      "5de97a43876f4fd08b97e50a7630514f",
      "1adcf463ca144a59bd2eb1f9d3defa2c",
      "9bcc5de447944cc9977010977c481911",
      "ebe6685c0b684ecabb9d3b40b710f61c",
      "4669814564cf46a098659bc3ad228739",
      "4af8e61ed01840eaa458d0156071618a",
      "1cf23cefb1d0401aac53ce86eae094ec",
      "4caf6a2509de46d29eb13f32b95dfdf1",
      "73e66624e5234104a06d5b4f3fab1b0d",
      "2ca8d1a180d742988c2a24dd72f3af5f",
      "fc80137b5ab64e72b4dbef9c6d559f64",
      "9f9ed03b37f04a3b8f324fcac6c3227b",
      "98171006365d43fd8d082ef38c2fde1f",
      "72e99d55b203495f95e1153f8263e5cf",
      "79dc763508544c218179147c2e19414d",
      "03d6ea67d5814386b49a19a7c1d0b3fd",
      "646eacc6130040b48f13ba03c79ba4e7",
      "19475a3a99f3457788dd163c9efdd46b",
      "b331721db3024bf2bf987a82c9ac2a69",
      "c7a3ea6140fc4c568289e69833d264d1",
      "14120bcc3b8149e3bf1bb6809d04ef14",
      "1d89e6b9eae142b0bb9d2525815c7222",
      "15b68a1865e342eebd63f900cbaa1956",
      "12cb0fa8ad354992b29cca82ce5e6bce",
      "b03a54afe9144306803345ecfa3f0f35",
      "153598edecae4c30aac247fd3123d5e9",
      "c6d4119c10e24787855f125fb83cac31",
      "6210f4c0d82244e69c8db266504db4bd",
      "f920094a60d641908ea7202d58315f92",
      "fb1a9a57bfe8484c80f264c12297fd0f",
      "d87f0c52691646bf9779fb3f35cab3a5",
      "85f96ff9eb1e448684ddb64c8bc897e5",
      "382e8970bf98403b9b4840fce786a91c",
      "5d4f0dec8f12481aaf0fa3c377a1dc3c",
      "2f9744f02d5f44bd9150c9a462f53ab7",
      "36c7b11553c54a9ea7cdfd57185c2b96",
      "b60060b0a09c409db75c8d509355f621",
      "9216e281538d4863b91a9f96797db3db",
      "cb06f2c82c824758a62bc85f373b9f07",
      "89b33ef989c64676b2e7d275e07715c4",
      "764e187af8c64af39275c1ce64b7c5e6",
      "c7adceded31a4e73815c6f47c2d63eb4",
      "12ee195a24fd46548f527e49eacb8735",
      "dd1c6f29ddde4735967a2475d0f2f8e3",
      "61510069d458442f83d024749ec549d7",
      "5b45e9caca474359b5b89cd7bf927817",
      "c82188732cd44c2f87090c8bc1bb9448",
      "50390d37c34e4efc873d35dfe39d68e3",
      "fe974be39fcf41c4ae85e636c15472ae",
      "14f87398248b495c9219702ad3cdffbb",
      "8cadc85e8d5f4f22897cfec3c14fa88f",
      "916ad5e851754f64b2f0e0952d94cc57",
      "6ef22e2c421049a4af946900639e6f63",
      "16fda30472b54a74969f19c6af4c5f52",
      "194194eef33845159b4a587a5fd3a354",
      "685c5682983a489bbb1f5a77f06626e5",
      "792ea727e62b40af8a83bb59acf16dd0",
      "bf1b0599fb5c4a8297deb8a67fabb5a6",
      "5bce4ac685c644b3b7e6f3cca6c861c2",
      "00001c2c3c5c47bda37a9723d3ebd89d",
      "cdfdd6be90b24eb9be7a189a37bef3be",
      "45fd406a992b4167a82abb7a5775f267",
      "435dba4da65b4e389a79512673db0d04",
      "975b075fc5ff49a2a306585ccfd45fc0",
      "eb5a3c8bb66c4846afb02fe5ce99d363",
      "046a3d58f251444689b87f35f6d90b18",
      "3c3bb91e84dd41c083ab72c2c9033d2d",
      "770bd291e5bb4ffcaed4d7a9e5ae154d",
      "1c3a36f321154a5aa11b4aaea7360efc",
      "e5fd143d86d24d2bb14402b5b09d5a62",
      "545a06f10aa24961886f18f48ffba7d5",
      "c5a6c5d41cac4364bfa5ac463333c66c",
      "37337dd7359741eb8183484a3e0cb6a1",
      "c769a636b17645c296ec770556b3cdb5",
      "fb3c518b7dff4bc189bfe156292cca57",
      "11662ad00ac34d1ab6b80cd4b3daee61",
      "aec68425f7414bff802958b2618dd6dd",
      "87ec64b4298a45c081b4403af5d2c00c",
      "96bd63560cdc4aecad4c62d60438e13b",
      "8e1839136fdd437faa8abfdebbb85306",
      "5fd4e5573c454321a495c81ef9c482bf",
      "f89c654537364e57af527f39e786a75b",
      "f6c806d5b4f844feb86ea0e0ee59b0b5",
      "3beeaafb36134748a23006aa4ea00628",
      "29b63a6d53704e729893a05a003a1684",
      "defc6a2737c0454a88be0884dc42602c",
      "01e388050c1545b0bea5141af3f5c6a6",
      "a656ecdc4cdc45eea4afb393cd72e6c6",
      "3585ee7e6df14f77b27948334aa3420b",
      "923554d713bc4a0ab8e2dcd649aa4ebe",
      "a214d4f3177f4d7694855813403251a4",
      "78f02872cc114842963877b11c217adb",
      "164d5deaaf9640639fe156b558fbc385",
      "cc7c7211894642eb81f075af6998a443",
      "ebfc939db7a541d2a4ac7faaa23e4147",
      "aa3c44e1f4664f0c99b502c04412a7f6",
      "eb602ae5a4f14b15abe8a3284c459eef",
      "31e82e021d5146dbbbb72e07fa8818e4",
      "f66500814db1447ca903ae20989f404b",
      "84ca66a18f904b6c99a596bf72547335",
      "20b917d5b2ef40528256752b37bcf860",
      "5c448d039ef845aea7502cd7ae0ae0f7",
      "b90f746ca9cb4755a30473e43dba56ae",
      "c6c9a16e26254ced956a40f8e5e00dc9",
      "8d0dd57a188440a2b843b34185583ce0",
      "a4b20b48244b46ddacb6f1b7c19e6c49",
      "442acebb6956477a82b9b119499b9715",
      "5f20361d0fad4e64b690e4518f2ff205",
      "aec135fbc76e4912af757a01ae9ba7bd",
      "b06bdbaa9fe1412facb7c1b1ef299600"
     ]
    },
    "id": "pgApT0ly9gPp",
    "outputId": "20841d50-6a44-40d7-e21e-354cd63c5b17"
   },
   "outputs": [],
   "source": [
    "llama_model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYoMLJ1OetL6",
    "outputId": "c4405afa-d453-4c56-882a-36af444ac4bf"
   },
   "outputs": [],
   "source": [
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llama_model,\n",
    "    tokenizer=llama_tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToIURiU7-kCo"
   },
   "source": [
    "# Tool creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9FTmtwc9dG_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def generate_synthetic_data_with_llama(schema: str, count: int):\n",
    "    print(\"Llama 3.1 model was called\")\n",
    "\n",
    "    llama_prompt = f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an expert synthetic data generator. Generate {count} rows of synthetic data following the provided schema.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Generate {count} rows of synthetic data following this schema:\n",
    "{schema}\n",
    "\n",
    "Return only valid JSON with keys:\n",
    "- \"data\": list of objects with feature:value pairs\n",
    "- \"model_type\": classification | regression | unsupervised\n",
    "- \"data_description\": short description of dataset topic\n",
    "\n",
    "!!! Do NOT output anything except the JSON !!!\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Generate response using Llama pipeline\n",
    "        response = llama_pipeline(\n",
    "            llama_prompt,\n",
    "            max_new_tokens=2000,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=llama_tokenizer.eos_token_id\n",
    "        )[0]['generated_text']\n",
    "\n",
    "        # Extract the assistant's last block\n",
    "        if \"<|start_header_id|>assistant<|end_header_id|>\" in response:\n",
    "            response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
    "\n",
    "        print(\"Raw model response ->\", response)\n",
    "\n",
    "        # --- Extract ONLY the JSON block ---\n",
    "        match = re.search(r\"\\{[\\s\\S]*\\}\", response)\n",
    "        if not match:\n",
    "            raise ValueError(\"No JSON object detected in model output.\")\n",
    "\n",
    "        json_str = match.group(0).strip()\n",
    "        print(\"Extracted JSON ->\", json_str)\n",
    "\n",
    "        # Parse JSON safely\n",
    "        data_json = json.loads(json_str)\n",
    "        data = data_json[\"data\"]\n",
    "        model_type = data_json[\"model_type\"]\n",
    "        data_description = data_json[\"data_description\"]\n",
    "\n",
    "        # Save to CSV (inside /content so it shows up in Colab’s file sidebar)\n",
    "        df = pd.DataFrame(data)\n",
    "        base_dir = \"/content/synthetic_data\"\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "        file_name = f\"{model_type}_{data_description.replace(' ', '_')}.csv\"\n",
    "        file_path = os.path.join(base_dir, file_name)\n",
    "\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "        return {\n",
    "            \"model_type\": model_type,\n",
    "            \"data_description\": data_description,\n",
    "            \"file_path\": file_path\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Llama generation: {str(e)}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"model_type\": \"Llama-3.1\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT5-FWkb-swA"
   },
   "source": [
    "# LLaMA conversation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoVsoyBA9nNm"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "# -------------------------------------------------\n",
    "#  MAIN STREAMING FUNCTION\n",
    "# -------------------------------------------------\n",
    "def llama_conversation_stream(user_prompt: str):\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        example_interaction = \"\"\"\n",
    "User: Generate 10 rows for demo.\n",
    "Assistant: Sure – here’s a preview of the dataset I will create …\n",
    "\n",
    "{\n",
    "  \"name\": \"generate_synthetic_data_with_llama\",\n",
    "  \"arguments\": {\n",
    "    \"schema\": \"{\\\"features\\\": [{\\\"name\\\": \\\"age\\\", \\\"type\\\": \\\"int\\\"}], \\\"target\\\": {\\\"name\\\": \\\"is_churn\\\", \\\"type\\\": \\\"bool\\\"}}\",\n",
    "    \"count\": 10\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "        messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful assistant.\\n\\n\"\n",
    "            \"When the user asks for synthetic data:\\n\"\n",
    "            \"1. First, respond conversationally to confirm that you understand and are eager to generate the data.\\n\"\n",
    "            \"2. Then, on a NEW line, output a **single JSON object** that calls the tool.\\n\"\n",
    "            \"   The JSON must have the following format:\\n\"\n",
    "            \"   {\\n\"\n",
    "            \"     \\\"name\\\": \\\"generate_synthetic_data_with_llama\\\",\\n\"\n",
    "            \"     \\\"arguments\\\": {\\n\"\n",
    "            \"       \\\"schema\\\": \\\"<escaped-schema-string>\\\",\\n\"\n",
    "            \"       \\\"count\\\": <int>\\n\"\n",
    "            \"     }\\n\"\n",
    "            \"   }\\n\\n\"\n",
    "            \"Important rules:\\n\"\n",
    "            \"• The 'schema' value must be a valid JSON string with quotes escaped (\\\\\\\").\\n\"\n",
    "            \"• Do NOT wrap the JSON in markdown.\\n\"\n",
    "            \"• Do NOT add any text after the JSON.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Example behaviour:\\n\" + example_interaction\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "        inputs = llama_tokenizer.apply_chat_template(\n",
    "            messages, return_tensors=\"pt\",\n",
    "            max_length=2048, truncation=True\n",
    "        ).to(llama_model.device)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "        streamer = TextIteratorStreamer(llama_tokenizer, skip_prompt=True)\n",
    "        gen_kwargs = dict(\n",
    "            inputs=inputs,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=1000,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=llama_tokenizer.eos_token_id\n",
    "        )\n",
    "        Thread(target=llama_model.generate, kwargs=gen_kwargs).start()\n",
    "\n",
    "        result = \"\"\n",
    "        func_name = None\n",
    "        args = None\n",
    "\n",
    "        for chunk in streamer:\n",
    "            result += chunk\n",
    "            yield result\n",
    "\n",
    "            clean = result.replace(\"<|eot_id|>\", \"\").strip()\n",
    "            m = re.search(r\"\\{[\\s\\S]*\\}\", clean)\n",
    "            if not m:\n",
    "                continue\n",
    "            try:\n",
    "                parsed = json.loads(m.group(0))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            if \"name\" in parsed and \"arguments\" in parsed:\n",
    "                func_name = parsed[\"name\"]\n",
    "                args = parsed[\"arguments\"]\n",
    "                break\n",
    "\n",
    "        # ----------------  TOOL EXECUTION  ---------------------------------\n",
    "        if func_name == \"generate_synthetic_data_with_llama\":\n",
    "            data = generate_synthetic_data_with_llama(**args)\n",
    "            followup = (\n",
    "                f\"\\n\\n✅ Tool executed successfully!\\n\"\n",
    "                f\"📂 File: {data['file_path']}\\n\"\n",
    "                f\"📊 Model: {data['model_type']}\\n\"\n",
    "                f\"📝 Description: {data['data_description']}\"\n",
    "            )\n",
    "            result += followup\n",
    "            yield result\n",
    "        elif func_name:\n",
    "            result += f\"\\n\\n⚠️ Tool '{func_name}' not implemented.\"\n",
    "            yield result\n",
    "        else:\n",
    "            result += \"\\n\\n⚠️ No valid tool call detected.\"\n",
    "            yield result\n",
    "\n",
    "    except Exception as exc:\n",
    "        yield f\"\\n\\n❌ Error: {exc}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ngT7Nxw_FO9"
   },
   "source": [
    "# Promplt example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WMLGLMv9qaB"
   },
   "outputs": [],
   "source": [
    "# \"Generate synthetic data for a customer churn classification problem with 10 rows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "vS2476kaRtDt",
    "outputId": "b4952494-2917-4d52-dbab-c939c9a5c2e3"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=llama_conversation_stream,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the past usage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Image](pictures/screenshot_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: While the model “Meta-Llama-3.1-8B-Instruct” is not fine-tuned for working with tools and this notebook may not be very practical, it serves as an excellent demonstration of how simple the concept of tools can be. We can implement a logic block that detects specific patterns in text (e.g., special tokens, JSON, etc.), extracts that portion from the response, and redirects it as a parameter to the corresponding tool. Then we can simply calling the conversational model again, pasting it the part of text before the tool call and providing it the tool call summary. Or we can simply hardcode the ending of the response after successful tool call!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
