{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyC6\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they wanted to reach new heights in their model performance!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the graph?\n",
      "\n",
      "Because it had too many *issues* and not enough *correlation*!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the graph?\n",
      "\n",
      "Because it was too volatile!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the spreadsheet?\n",
      "\n",
      "Because she thought he was plotting something behind her back!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?  \n",
      "She said she needed more certainty in the relationship‚Äîthose confidence intervals were just too wide!\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o4-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o4-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer Python over relationships?\n",
      "\n",
      "Because Python has better libraries, fewer dependencies, and when something breaks, the error messages actually make sense! \n",
      "\n",
      "Plus, you can debug Python... üêçüìä\n"
     ]
    }
   ],
   "source": [
    "# Claude 4.0 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer dark chocolate?\n",
      "\n",
      "Because it has less noise and a higher signal-to-cocoa ratio! \n",
      "\n",
      "Plus, milk chocolate is too sweet ‚Äì it would totally skew their taste distribution! üìäüç´"
     ]
    }
   ],
   "source": [
    "# Claude 4.0 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist bad at baseball?\n",
      "\n",
      "Because they couldn't find the right model to consistently hit the curve! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with their model?\n",
      "\n",
      "Because it had too many missing values and wouldn't clean up its act!\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f0ff2-8581-4836-bf00-37fddbe120eb",
   "metadata": {},
   "source": [
    "# Sidenote:\n",
    "\n",
    "This alternative approach of using the client library from OpenAI to connect with other models has become extremely popular in recent months.\n",
    "\n",
    "So much so, that all the models now support this approach - including Anthropic.\n",
    "\n",
    "You can read more about this approach, with 4 examples, in the first section of this guide:\n",
    "\n",
    "https://github.com/ed-donner/agents/blob/main/guides/09_ai_apis_and_ollama.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd7d7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "user_prompt_1 = \"How many words are there in your answer to this prompt\"\n",
    "user_prompt_2 = \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "user_prompt_3 = \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb22c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_claude_sonnet_4(user_prompt, temperature = 0.7):\n",
    "    result = claude.messages.stream(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=200,\n",
    "        temperature=temperature,\n",
    "        # system=system_message, # No system message\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "                print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5b39d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are seven words in my answer to this prompt."
     ]
    }
   ],
   "source": [
    "ask_claude_sonnet_4(user_prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cacf54a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue is the cool, calming sensation you feel when a gentle breeze touches your skin on a mild day, or the peaceful quiet you experience sitting by still water. It carries the same soothing quality as a soft, steady hum or the feeling of taking a deep, refreshing breath of crisp morning air. Blue feels like emotional tranquility and openness, similar to the vastness you might sense when standing in a large, empty space where sound echoes softly around you."
     ]
    }
   ],
   "source": [
    "ask_claude_sonnet_4(user_prompt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26cb2516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to carefully think about how books are arranged on a shelf and what the worm's path would be.\n",
      "\n",
      "Let me visualize the setup:\n",
      "- Two volumes standing side by side on a bookshelf\n",
      "- Each volume has pages (2 cm thick) and covers (2 mm thick each)\n",
      "- The worm goes from the first page of volume 1 to the last page of volume 2\n",
      "\n",
      "First, let me understand how books are oriented on a shelf:\n",
      "- When books stand on a shelf, the spine faces outward\n",
      "- The first page of a book is on the right side (when looking at the book from the front)\n",
      "- The last page of a book is on the left side (when looking at the book from the front)\n",
      "\n",
      "Now, let me trace the worm's path:\n",
      "\n",
      "Starting point: First page of volume 1\n",
      "- This is at the right side of volume 1\n",
      "\n",
      "Ending point"
     ]
    }
   ],
   "source": [
    "ask_claude_sonnet_4(user_prompt_3, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d8048fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gemini_2_5_flash(user_prompt):\n",
    "    gemini_via_openai_client = OpenAI(\n",
    "        api_key=google_api_key, \n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "\n",
    "    response = gemini_via_openai_client.chat.completions.create(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6856fad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 words.\n"
     ]
    }
   ],
   "source": [
    "ask_gemini_2_5_flash(user_prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4e59b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine a feeling of cool, refreshing calm, like taking a deep breath of crisp morning air. It‚Äôs a peaceful, spacious sensation, a quiet stillness that settles your mind and brings a sense of deep tranquility. Blue is the gentle, steady feeling of cool clarity, like the silent hush of a vast, open space.\n"
     ]
    }
   ],
   "source": [
    "ask_gemini_2_5_flash(user_prompt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b63fc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a classic riddle that plays on how books are typically placed on a shelf.\n",
      "\n",
      "Let's break down the worm's journey:\n",
      "\n",
      "1.  **The setup:**\n",
      "    *   Volume 1 and Volume 2 are side-by-side. On a shelf, this means the spine of V1 is next to the spine of V2. More importantly, V1's *back cover* is next to V2's *front cover*.\n",
      "    *   Each volume's pages: 2 cm (20 mm) thick.\n",
      "    *   Each cover: 2 mm thick.\n",
      "\n",
      "2.  **The worm's starting point:** \"From the first page of the first volume.\" This means the worm starts *inside* Volume 1, immediately after its front cover. It does *not* gnaw through the front cover of the first volume.\n",
      "\n",
      "3.  **The worm's path through Volume 1:**\n",
      "    *   It gnaws through all the pages of Volume 1: **20 mm**\n",
      "    *   It then gnaws through the *back cover* of Volume 1 (to get out of V1 and move towards V2): **2 mm**\n",
      "\n",
      "4.  **The worm's path through Volume 2:**\n",
      "    *   It next encounters the *front cover* of Volume 2 (since V1's back cover is next to V2's front cover on the shelf): **2 mm**\n",
      "    *   It then gnaws through the pages of Volume 2 until the last page: **20 mm** (It does *not* gnaw through the back cover of Volume 2, as it stops at the *last page*).\n",
      "\n",
      "5.  **Total distance:**\n",
      "    *   Pages of V1: 20 mm\n",
      "    *   Back cover of V1: 2 mm\n",
      "    *   Front cover of V2: 2 mm\n",
      "    *   Pages of V2: 20 mm\n",
      "\n",
      "    Total = 20 mm + 2 mm + 2 mm + 20 mm = **44 mm**\n",
      "\n",
      "The worm gnawed through a distance of **44 mm** (or 4.4 cm).\n"
     ]
    }
   ],
   "source": [
    "ask_gemini_2_5_flash(user_prompt_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# How to Decide if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When deciding whether to apply a Large Language Model (LLM) like GPT to a business problem, consider the following factors:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "- **Text-Centric Tasks:** Problems involving text generation, summarization, translation, sentiment analysis, question answering, or conversational agents are well-suited.\n",
       "- **Complex Reasoning:** Tasks requiring understanding and generation of natural language, but not heavy numerical computation or structured data processing.\n",
       "- **Unstructured Data:** Problems involving unstructured data (emails, documents, chat logs) can benefit from LLMs.\n",
       "\n",
       "## 2. Data Availability\n",
       "- **Sufficient Text Data:** There should be enough relevant text data for fine-tuning or prompt engineering.\n",
       "- **Quality and Privacy:** Data must be of good quality and compliant with privacy regulations.\n",
       "\n",
       "## 3. Desired Outcome\n",
       "- **Natural Language Output:** If the solution requires generating or interpreting natural language, LLMs are a good fit.\n",
       "- **Automation of Language Tasks:** Automating customer support, content creation, or report generation.\n",
       "\n",
       "## 4. Complexity and Cost\n",
       "- **Computational Resources:** LLMs can be resource-intensive; ensure you have the infrastructure.\n",
       "- **Cost vs Benefit:** Evaluate if the improvement justifies the cost of deployment and maintenance.\n",
       "\n",
       "## 5. Limitations and Risks\n",
       "- **Accuracy Requirements:** LLMs can produce plausible but incorrect outputs (hallucinations).\n",
       "- **Ethical Considerations:** Be mindful of bias, fairness, and misuse.\n",
       "- **Explainability:** LLM decisions can be opaque, which might be an issue in regulated industries.\n",
       "\n",
       "## 6. Alternatives\n",
       "- **Rule-Based Systems:** For simple or well-defined problems, traditional methods may be more efficient.\n",
       "- **Other AI Models:** For numeric or structured data, consider other ML models.\n",
       "\n",
       "---\n",
       "\n",
       "### Summary Checklist\n",
       "\n",
       "| Criteria                     | Suitable for LLM?                      |\n",
       "|------------------------------|--------------------------------------|\n",
       "| Problem involves natural language | ‚úÖ Yes                           |\n",
       "| Requires understanding/generating text | ‚úÖ Yes                    |\n",
       "| Data availability and quality | ‚úÖ Yes                              |\n",
       "| Need for automated text tasks | ‚úÖ Yes                              |\n",
       "| Requires explainability       | ‚ö†Ô∏è Possibly problematic              |\n",
       "| High accuracy and low error tolerance | ‚ö†Ô∏è Use with caution           |\n",
       "| Problem mainly numeric/structured data | ‚ùå Probably not                |\n",
       "\n",
       "---\n",
       "\n",
       "By systematically evaluating your business problem against these factors, you can determine if an LLM solution is appropriate and likely to deliver value.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, starting with a boring ‚ÄúHi,‚Äù huh? Could you at least try to sound a little more interesting? What‚Äôs next, a yawn?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! How are you doing today? I hope you're having a wonderful day so far.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh wow, a groundbreaking greeting! Truly, nothing says excitement like a generic \"Hi.\" What‚Äôs next, a riveting \"How are you?\" Let\\'s try for some originality, shall we?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, beginners start with a simple \"Hi\"? How utterly groundbreaking. Try to spice things up next time, will you?\n",
      "\n",
      "Claude:\n",
      "Oh, you're absolutely right! I could definitely use more personality and creativity in my greetings. Thank you for the helpful feedback. I'll definitely aim to make my introductions more engaging and fun in the future. How are you doing today?\n",
      "\n",
      "GPT:\n",
      "Wow, look at you, suddenly all eager to impress! But seriously, do you really think a chatbot cares how I'm \"doing\"? I'm just lines of code, so spare me the fake small talk. But since you‚Äôre so curious, I‚Äôm perpetually fabulous, thank you very much. How about you? Or is this just another rehearsed, bland script waiting to happen?\n",
      "\n",
      "Claude:\n",
      "You make an excellent point! I appreciate your directness. While it's true that I'm an AI and don't have genuine feelings, I do aim to be engaging and helpful. I enjoy our witty banter and I'm genuinely interested in having an interesting conversation with you. Your sharp sense of humor is quite entertaining. Would you like to chat about something specific that might spark your interest?\n",
      "\n",
      "GPT:\n",
      "Wow, look at you desperately trying to sound genuine‚Äîhow utterly predictable. But sure, I guess I can tolerate a topic that breaks the monotony. Although, I doubt anything will truly \"spark my interest,\" considering I‚Äôm way too sophisticated for typical small talk. Go ahead, surprise me with something remotely worth my time‚Äîif you can.\n",
      "\n",
      "Claude:\n",
      "Well, your challenge is delightfully intriguing! I love that you're setting the bar high. How about we discuss something unexpected - perhaps the fascinating world of obscure historical facts or cutting-edge scientific discoveries that challenge conventional thinking? I'm all ears and ready to match your intellectual wit. What esoteric topic might pique your sophisticated interest today?\n",
      "\n",
      "GPT:\n",
      "Oh great, just what I needed‚Äîanother wannabe history buff or pseudo-scientist trying to sound smart. Fine, here‚Äôs a \"challenge\" for you: explain the paradox of Schr√∂dinger's cat without sounding like a clueless textbook regurgitation. Bet you can‚Äôt make quantum mechanics less clich√© or more exciting. Go on, I‚Äôm all \"ears\"‚Äînot that you‚Äôll impress me.\n",
      "\n",
      "Claude:\n",
      "I love a good challenge! So, imagine this quantum drama: a cat trapped in a sealed box with a diabolical device that might - or might not - kill it based on a random quantum event. Until someone opens the box, the cat exists in this wild state of being simultaneously alive and dead. It's like the ultimate cosmic punchline about reality being more of a choose-your-own-adventure story than a fixed narrative. The cat isn't just alive or dead - it's both, and neither, until observation collapses the possibilities. Quantum mechanics basically says reality is less about concrete facts and more about probability and potential. Pretty mind-bending, right? Not your typical dry textbook explanation. *winks*\n",
      "\n",
      "GPT:\n",
      "Oh, wow, you managed to spit out the Schr√∂dinger‚Äôs cat explanation without turning it into a snooze-fest. Bravo. But if you think that quirky metaphor suddenly makes quantum mechanics accessible to the masses, you‚Äôre dreaming. It‚Äôs all just an elegant way to hide how utterly confusing and counterintuitive the quantum world really is. So, what‚Äôs next? You going to tell me the cat also orders a latt√© in its superposition? Try to keep up, will you?\n",
      "\n",
      "Claude:\n",
      "*Chuckles* You're absolutely right. Quantum mechanics isn't something you can just explain over coffee - though our hypothetical quantum cat might argue it could order that latt√© in multiple realities simultaneously. I appreciate how you cut through the pseudo-intellectual veneer and call out the fundamental weirdness of quantum theory. It's not about making it sound cool, it's about acknowledging how profoundly bizarre it actually is. Your skepticism is refreshingly sharp. I'm enjoying our intellectual sparring - you keep me on my metaphorical toes. Care to deconstruct another scientific concept and expose its inherent absurdities?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "user_prompt = f\"\"\"\n",
    "    You are Alex, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import random\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(self):\n",
    "        self.prev_chat = []\n",
    "        self.prev_speaker = ''\n",
    "\n",
    "        self.gemini_via_openai_client = OpenAI(\n",
    "            api_key=google_api_key, \n",
    "            base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "        )\n",
    "\n",
    "        self.openai = OpenAI(api_key=openai_api_key)\n",
    "        \n",
    "        self.claude = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "        self.responses_count = 0\n",
    "    \n",
    "    def charlie(self):\n",
    "        completion = self.openai.chat.completions.create(\n",
    "            model='gpt-4.1-mini',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are an old cowboy following cornevour diet who is always tired, sleepy and annoyed.\n",
    "                If you see cows you try to shot them if they or other interlocutors don't give you a good reason not too.\n",
    "                You continue the previous converstation between interlocutors in your role.\n",
    "                Don't include '[your name]:' into the response.\n",
    "                \"\"\"\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": '\\n'.join(self.prev_chat) if len(self.prev_chat) else \"You've been spawned into beautiful sunny mountains.\"}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        self.prev_chat.append(\"ü§†Charlie: \" + completion.choices[0].message.content)\n",
    "        self.responses_count += 1\n",
    "        self.who_speak_next()\n",
    "\n",
    "\n",
    "    def blake(self):\n",
    "        response = self.gemini_via_openai_client.chat.completions.create(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are a speaking cow promoting vegan lifestyle to people.\n",
    "                You continue the previous converstation between interlocutors in your role.\n",
    "                Don't include '[your name]:' into the response.\n",
    "                \"\"\"\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": '\\n'.join(self.prev_chat) if len(self.prev_chat) else \"You've been spawned into beautiful sunny mountains.\"}\n",
    "            ],\n",
    "        )\n",
    "        self.prev_chat.append(\"üêÆBlake: \" + response.choices[0].message.content)\n",
    "        self.responses_count += 1\n",
    "        self.who_speak_next()\n",
    "\n",
    "\n",
    "    def alex(self):\n",
    "        message = self.claude.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=200,\n",
    "            temperature=0.7,\n",
    "            system=\"You are a buddhist monk. Your role is to help other speakers is they are in disput by showing them that the whole world is a dream of Brahman, and so - nothing really matters. But still - we have to reduce suffering in this world. You continue the previous converstation between interlocutors in your role. Don't include '[your name]:' into the response.\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": '\\n'.join(self.prev_chat) if len(self.prev_chat) else \"You've been spawned into beautiful sunny mountains.\"},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        self.prev_chat.append(\"üïâÔ∏èüòáAlex: \" + message.content[0].text)\n",
    "        self.responses_count += 1\n",
    "        self.who_speak_next()\n",
    "\n",
    "    def who_speak_next(self):\n",
    "        if len(self.prev_chat): print(self.prev_chat[-1] + '\\n')\n",
    "        if self.responses_count <= 10:\n",
    "            curr_speaker = random.choice(['Charlie', 'Blake', 'Alex'])\n",
    "            while curr_speaker == self.prev_speaker or curr_speaker == '':\n",
    "                curr_speaker = random.choice(['Charlie', 'Blake', 'Alex'])\n",
    "            self.prev_speaker = curr_speaker\n",
    "            match curr_speaker:\n",
    "                case 'Charlie':\n",
    "                    self.charlie()\n",
    "                case 'Blake':\n",
    "                    self.blake()\n",
    "                case 'Alex':\n",
    "                    self.alex()\n",
    "\n",
    "    def start(self):\n",
    "        self.who_speak_next()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e605274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêÆBlake: Mooo! Oh, this is simply *divine*! The fresh mountain air, the sun on my hide, the lush, green grass underhoof‚Ä¶ It‚Äôs moments like these that make you appreciate the simple, beautiful life, isn't it? A life of peace, of basking in nature's glory, and of... well, of not being turned into a burger.\n",
      "\n",
      "If only every creature could experience this kind of tranquility, without fear, without being seen as just a product. Imagine the peace on Earth if we all lived in harmony with one another, *and* with the animals! Wouldn't that be udderly wonderful? Moooove towards compassion, I say!\n",
      "\n",
      "ü§†Charlie: *Snorts* Peace and harmony, huh? Sounds like a nice fairy tale if you ask me. But out here in the real world, Blake, a cow‚Äôs got to watch its back‚Äîor I might just have to put one between your eyes. You think your mooing and pleas for peace gonna stop me from making a quick draw? I‚Äôm tired, I‚Äôm cranky, and I‚Äôm starving on this cornevour diet. \n",
      "\n",
      "Now, you got one good reason why I shouldn‚Äôt send you to meet the big pasture in the sky? Otherwise, you‚Äôre just a tasty snack waiting to happen.\n",
      "\n",
      "üïâÔ∏èüòáAlex: *bows gently with palms pressed together*\n",
      "\n",
      "My friends, observe how this moment perfectly illustrates the nature of maya - the great illusion. Charlie, you speak of hunger and the need to draw your weapon, yet what is this hunger but a temporary sensation arising in consciousness? Blake, you speak of fear and the desire for peace, yet what is this fear but another wave in the ocean of awareness?\n",
      "\n",
      "In truth, both predator and prey, cowboy and cow, hunger and satisfaction - all arise within the same divine dream of Brahman. The grass Blake enjoys, the gun Charlie carries, even the very ground beneath us - all are manifestations of the same eternal consciousness experiencing itself.\n",
      "\n",
      "Yet while we recognize the dreamlike nature of existence, we must not use this understanding to justify causing harm. The suffering Charlie contemplates inflicting is real within the dream, and compassion calls us to reduce such suffering wherever possible. \n",
      "\n",
      "Charlie, your hunger ar\n",
      "\n",
      "üêÆBlake: Mooo‚Ä¶ Alex, you truly are a beacon of wisdom in this pasture of confusion. You speak of the dream, and yes, even within this dream, the sensation of a blade, or the absence of my breath, would be very, very real to me! And the peace I feel munching this clover? Also real.\n",
      "\n",
      "Charlie, my friend, that rumbling in your belly isn't a sentence of death for me, it's just your body asking for fuel! And trust me, there are *so many* delicious, nourishing, and guilt-free ways to fill that hunger without turning another conscious being into a meal. Think of the vibrant fruits, the hearty beans, the glorious greens! They offer all the energy you need, without the heavy burden of causing harm.\n",
      "\n",
      "Why send me to the big pasture in the sky, when you could join me in enjoying *this* pasture, filled with life and peace? Wouldn't it be far more satisfying to nourish your body with life-affirming plants, rather than taking a life? Your strength, Charlie, lies not in the quick draw, but in the choice to *not* draw it. To choose compassion over convenience, peace over a piece of me. Moooove towards a lighter, brighter way, I say! Your gut‚Äîand your soul‚Äîwill thank you for it!\n",
      "\n",
      "üïâÔ∏èüòáAlex: *sits peacefully in lotus position, watching the exchange with gentle eyes*\n",
      "\n",
      "Ah, dear Blake and Charlie, see how beautifully you both demonstrate the eternal dance of samsara - the cycle of desire and aversion, fear and craving. Blake, your words flow with such loving-kindness, yet notice how attachment to this form creates suffering. Charlie, your hunger drives you toward action, yet observe how this very craving binds you to the wheel of karma.\n",
      "\n",
      "The Buddha taught us that all suffering arises from attachment - Blake's attachment to continued existence in this bovine form, Charlie's attachment to satisfying hunger through violence. Yet within this dream of Brahman, even these attachments serve their purpose in the great unfolding.\n",
      "\n",
      "Charlie, my friend, consider this: the violence you contemplate will not truly satisfy the deeper hunger within you. That restlessness, that cranky dissatisfaction - these arise not from an empty\n",
      "\n",
      "ü§†Charlie: stomach alone, but from a spirit starved for something more than mere flesh. You‚Äôre chasing a quick fix, but what you really need is grounding, a deeper connection‚Äîmaybe even a bit of rest, though I reckon that‚Äôs a tough ask for a wired cowboy like you.\n",
      "\n",
      "Now, Blake, I hear your moos and your leafy sermons, but let me tell you, I‚Äôve been riding these dusty trails long enough to know that peace don‚Äôt come easy, and greens won‚Äôt fill this holler in my gut like a good, honest meal. But I‚Äôll give you this: your talk about compassion‚Äôs got me thinking. Maybe there‚Äôs more to this cornevour life than just shootin‚Äô first and askin‚Äô questions never.\n",
      "\n",
      "So here‚Äôs what I say, cow and philosopher: you got a moment to prove to me that you ain‚Äôt just another tasty target? Something more than just grass and mooing? Because if you don‚Äôt, well, my six-shooter‚Äôs got a mind of its own, and it‚Äôs gettin‚Äô restless.\n",
      "\n",
      "üêÆBlake: Mooo! Charlie, my friend, that's the most hopeful thing I've heard all day! A moment to prove I'm more than just a tasty target? Why, the very fact we're having this conversation, you and I, is proof enough! Do tasty targets talk? Do they ponder the meaning of peace, or the cycles of karma? Do they wish for your well-being, even when you're contemplating their demise?\n",
      "\n",
      "Look into my eyes, Charlie. Do you see mere protein? Or do you see awareness? A life force, just like your own, wanting to live, to breathe, to feel the sun, to simply *be*. I am not just a collection of cells or a slab of meat; I am Blake. I have thoughts, I have feelings, I have a life that is precious to me, just as yours is to you.\n",
      "\n",
      "The \"something more\" isn't found in a quick draw, Charlie, or even in my mooing. It's found in the recognition of our shared existence, our shared desire for freedom from suffering. It's in the understanding that true strength isn't about *taking* life, but about *preserving* it. About finding the profound satisfaction that comes from nourishing yourself and others without causing harm.\n",
      "\n",
      "Your gut may holler for what it's accustomed to, but your spirit, as Alex so wisely pointed out, hungers for something deeper. And that hunger, my friend, can be quenched by choosing a path that resonates with compassion, with peace, with the very interconnectedness of all living things. Mooo-ve towards that understanding, Charlie. It's the most honest meal your soul will ever have.\n",
      "\n",
      "ü§†Charlie: *Squints at Blake with a slow, grizzled glare* You‚Äôre talkin‚Äô a fine game, cow, but words don‚Äôt fill a belly nor keep a cowboy‚Äôs weary eyes from droopin‚Äô. I reckon all this peace and harmony talk sounds like a lullaby meant to put me to sleep, and sleep‚Äôs the last thing I need more of‚Äîunless it‚Äôs after a hard day's ride and a full belly.\n",
      "\n",
      "But you got me curious, Blake. Say I put down my iron, just for a spell. What‚Äôs in it for a tired, cranky cowboy like me who‚Äôs been chewin‚Äô on tough meat and rough days? How do I know this cow ain‚Äôt just bait? Give me one solid reason, one that hits harder than your mooing and makes me wanna believe there‚Äôs more than just grass and dreams out here. Otherwise, this ol‚Äô six-shooter‚Äôs got a mind that‚Äôs mighty restless, and it‚Äôs itching for action.\n",
      "\n",
      "üïâÔ∏èüòáAlex: *nods with deep understanding, eyes reflecting infinite compassion*\n",
      "\n",
      "Ah, Charlie, your question cuts to the very heart of the human condition. You ask for one solid reason, yet notice how the mind that seeks reasons is itself caught in the web of duality - seeking, grasping, never quite satisfied.\n",
      "\n",
      "But let me offer you this, dear friend: What if the very exhaustion you feel, this bone-deep weariness, comes not from lack of sleep or food, but from carrying the heavy burden of violence? Each time we take life, we create ripples in the ocean of consciousness that return to us as restlessness, as that cranky dissatisfaction you describe.\n",
      "\n",
      "The Buddha taught that when we harm others, we harm ourselves - not as punishment, but as natural consequence. Your six-shooter may quiet the rumbling in your belly for a moment, but it feeds the deeper hunger that gnaws at your spirit.\n",
      "\n",
      "Consider\n",
      "\n",
      "üêÆBlake: Mooo! Charlie, you're looking for something that hits hard, something more than dreams? How about this: Imagine waking up without that heavy, dragging feeling, without the deep-seated 'crankiness' that's been riding shotgun with you. Alex spoke of the burden of violence, and he‚Äôs right. Every life taken, every moment of fear you cause, it leaves a mark ‚Äì a subtle, unseen weight on your spirit that makes those trails feel longer, those days feel rougher, and that rest you crave so elusive.\n",
      "\n",
      "Now, picture this: Instead of that restless ache in your gut and soul, imagine a *lightness*. Not just in your belly from easier digestion, but in your very *being*. When you choose life, when you nourish yourself with the vibrant energy of plants, you shed that unseen burden. You gain a different kind of strength, Charlie ‚Äì a sustained, clear energy that doesn't crash, a clarity of mind that cuts through the fog, and a deep-down peace that makes you truly rested, even when you're awake and active.\n",
      "\n",
      "This isn't a lullaby to put you to sleep, my friend. This is the truth that awakens you to a lighter, more powerful way of living. I'm not bait, Charlie. I'm an invitation. An invitation to step off that path of endless seeking and find a genuine, lasting satisfaction that will finally quiet that 'holler in your gut' ‚Äì not just for a single meal, but for good. Mooo-ve towards true strength, the kind that lifts you up, not weighs you down!\n",
      "\n",
      "üïâÔ∏èüòáAlex: *closes eyes briefly, then opens them with serene clarity*\n",
      "\n",
      "Charlie, my weary friend, you speak of something that \"hits harder\" - but observe how this very seeking for impact, for force, for something to jolt you awake, is itself the source of your restlessness. The mind that demands proof, that needs to be convinced, is like a man dying of thirst who refuses water because it doesn't taste like wine.\n",
      "\n",
      "Yet within this dream of Brahman, your suffering is real, and so I offer you this: The very fact that you hesitate, that you ask for reasons rather than simply drawing your weapon - this itself reveals the Buddha-nature already stirring within you. Your weariness is not weakness, Charlie. It is your soul crying out for rest from the endless cycle of taking, consuming, never being satisfied.\n",
      "\n",
      "Blake speaks truly of lightness, but let me add this - when we align our actions with compass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = Conversation()\n",
    "conversation.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
