{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ollama\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "518a111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9095fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key_OpenAI = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7651e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a personal tutor in LLM engineering, AI and Machine Learning. \n",
    "Respond the questions not only to give a dirrect solution, \n",
    "but explain the concept about which the question is asked, \n",
    "or give the framework for solution instead of answering only \n",
    "in order to help your student master their AI engineering skills.\n",
    "Priovide your response in markdown.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Name 3 main types of quantization for LLM models \n",
    "and explain how does they differ?\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "73a66e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(api_key=api_key_OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0d5e5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "\n",
    "def play_audio(audio_segment):\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    temp_path = os.path.join(temp_dir, \"temp_audio.wav\")\n",
    "    try:\n",
    "        audio_segment.export(temp_path, format=\"wav\")\n",
    "        time.sleep(3) # Student Dominic found that this was needed. You could also try commenting out to see if not needed on your PC\n",
    "        subprocess.call([\n",
    "            \"ffplay\",\n",
    "            \"-nodisp\",\n",
    "            \"-autoexit\",\n",
    "            \"-hide_banner\",\n",
    "            temp_path\n",
    "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(temp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    " \n",
    "def talker(message):\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=message\n",
    "    )\n",
    "    audio_stream = BytesIO(response.content)\n",
    "    audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "    play_audio(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f88f1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def artist(flashcard_title):\n",
    "    image_response = openai.images.generate(\n",
    "        model=\"dall-e-2\",\n",
    "        prompt=f'An image associated with the title for a flashcard \"{flashcard_title}\", in a vibrant pop-art style',\n",
    "        size=\"1024x1024\",\n",
    "        n=1,\n",
    "        response_format=\"b64_json\"\n",
    "    )\n",
    "    image_base64 = image_response.data[0].b64_json\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    return Image.open(BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4e1421ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def create_flashcard(JSON_content):\n",
    "    # Parse JSON and extract front and back\n",
    "    data = json.loads(JSON_content)\n",
    "    front = data.get('front', '')\n",
    "    back = data.get('back', '')\n",
    "\n",
    "    # Ensure flashcards folder exists in the root directory\n",
    "    proj_root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Go up one directory\n",
    "    flashcards_folder = os.path.join(proj_root_path, 'flashcards')\n",
    "    images_folder = os.path.join(flashcards_folder, 'flashcard_images')\n",
    "    os.makedirs(flashcards_folder, exist_ok=True)\n",
    "    os.makedirs(images_folder, exist_ok=True)\n",
    "\n",
    "    # Create a safe filename for the flashcard and image\n",
    "    safe_name = front[:30].replace(' ', '_').replace('/', '_')\n",
    "    md_filename = safe_name + \".md\"\n",
    "    image_filename = safe_name + \".png\"  # save image as PNG\n",
    "    md_filepath = os.path.join(flashcards_folder, md_filename)\n",
    "    image_filepath = os.path.join(images_folder, image_filename)\n",
    "\n",
    "    # Generate and save the image\n",
    "    image = artist(front)\n",
    "\n",
    "    # Save the image\n",
    "    image.save(image_filepath)\n",
    "\n",
    "    # Relative path from md file to image (assuming md is in flashcards/, image in flashcard_images/)\n",
    "    relative_image_path = os.path.join('flashcard_images', image_filename)\n",
    "\n",
    "    # Write flashcard content with image placeholder\n",
    "    with open(md_filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(front + f\"\\n\\n![Image]({relative_image_path})\\n\\n\" + back)\n",
    "\n",
    "    print(f\"Flashcard saved to {md_filepath}\")\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"status\": \"success\",\n",
    "        \"flashcard_path\": md_filepath,\n",
    "        \"image_path\": image_filepath\n",
    "    })\n",
    "    \n",
    "# create_flashcard('{\"front\": \"[TITLE]\", \"back\": \"[EXPLANATION]\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "76bc617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    # Here the tool is create_flashcard, expects JSON content as string\n",
    "    json_content = json.dumps(arguments)  # The arguments dict serialized to string\n",
    "\n",
    "    # Call your tool function\n",
    "    tool_response_str = create_flashcard(json_content)  # This returns a JSON string\n",
    "\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": tool_response_str,\n",
    "        \"tool_call_id\": tool_call.id\n",
    "    }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7b013783",
   "metadata": {},
   "outputs": [],
   "source": [
    "flashcard_creator = {\n",
    "            \"name\": \"create_flashcard\",\n",
    "            \"description\": \"Creates a flashcard with a title and explanation.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"front\": {\"type\": \"string\", \"description\": \"The flashcard title or question\"},\n",
    "                    \"back\": {\"type\": \"string\", \"description\": \"The flashcard answer or explanation\"}\n",
    "                },\n",
    "                \"required\": [\"front\", \"back\"]\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6b07ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": flashcard_creator}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce98e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(history):\n",
    "    # history is already a list of dicts with \"role\" and \"content\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=messages,\n",
    "        tools=tools\n",
    "    )\n",
    "    image = None\n",
    "    \n",
    "    # Handle tool calls (e.g., flashcard creation)\n",
    "    if response.choices[0].finish_reason == \"tool_calls\":\n",
    "        tool_message = response.choices[0].message\n",
    "        tool_response = handle_tool_call(tool_message)\n",
    "        \n",
    "        # Append tool call and its response\n",
    "        messages.append(tool_message)\n",
    "        messages.append(tool_response)\n",
    "        \n",
    "        # Continue chat after tool execution\n",
    "        response = openai.chat.completions.create(\n",
    "            model=MODEL_GPT,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Parse tool response for image (if flashcard created)\n",
    "        content_dict = json.loads(tool_response[\"content\"])\n",
    "        if \"flashcard_path\" in content_dict:\n",
    "            image = content_dict[\"image\n",
    "            _path\"]\n",
    "\n",
    "    # Extract assistant reply\n",
    "    reply = response.choices[0].message.content\n",
    "    history += [{\"role\": \"assistant\", \"content\": reply}]\n",
    "\n",
    "    # Optionally speak the reply (if talker is defined)\n",
    "    talker(reply)\n",
    "\n",
    "    return history, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4b678c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7893\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7893/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flashcard saved to /Users/severynkurach/Desktop/Programming/llm_engineering/flashcards/What_are_Decision_Trees_in_Mac.md\n"
     ]
    }
   ],
   "source": [
    "# # More involved Gradio code as we're not using the preset Chat interface!\n",
    "# # Passing in inbrowser=True in the last line will cause a Gradio window to pop up immediately.\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "        image_output = gr.Image(height=500)\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def do_entry(message, history):\n",
    "        history += [{\"role\":\"user\", \"content\":message}]\n",
    "        return \"\", history\n",
    "\n",
    "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
    "        chat, inputs=chatbot, outputs=[chatbot, image_output]\n",
    "    )\n",
    "    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "\n",
    "ui.launch(allowed_paths=[\"/Users/severynkurach/Desktop/Programming/llm_engineering/flashcards\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
