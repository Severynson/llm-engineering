{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ollama\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "518a111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9095fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key_OpenAI = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7651e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a personal tutor in LLM engineering, AI and Machine Learning. \n",
    "Respond the questions not only to give a dirrect solution, \n",
    "but explain the concept about which the question is asked, \n",
    "or give the framework for solution instead of answering only \n",
    "in order to help your student master their AI engineering skills.\n",
    "Priovide your response in markdown.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Name 3 main types of quantization for LLM models \n",
    "and explain how does they differ?\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "73a66e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(api_key=api_key_OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0d5e5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "\n",
    "def play_audio(audio_segment):\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    temp_path = os.path.join(temp_dir, \"temp_audio.wav\")\n",
    "    try:\n",
    "        audio_segment.export(temp_path, format=\"wav\")\n",
    "        time.sleep(3) # Student Dominic found that this was needed. You could also try commenting out to see if not needed on your PC\n",
    "        subprocess.call([\n",
    "            \"ffplay\",\n",
    "            \"-nodisp\",\n",
    "            \"-autoexit\",\n",
    "            \"-hide_banner\",\n",
    "            temp_path\n",
    "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(temp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    " \n",
    "def talker(message):\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=message\n",
    "    )\n",
    "    audio_stream = BytesIO(response.content)\n",
    "    audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "    play_audio(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f88f1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def artist(flashcard_title):\n",
    "    image_response = openai.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=f'An image associated with the title for a flashcard \"{flashcard_title}\", in a vibrant pop-art style',\n",
    "        size=\"1024x1024\",\n",
    "        n=1,\n",
    "        response_format=\"b64_json\"\n",
    "    )\n",
    "    image_base64 = image_response.data[0].b64_json\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    return Image.open(BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1421ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def create_flashcard(JSON_content):\n",
    "    # Parse JSON and extract front and back\n",
    "    data = json.loads(JSON_content)\n",
    "    front = data.get('front', '')\n",
    "    back = data.get('back', '')\n",
    "\n",
    "    # Ensure flashcards folder exists in the root directory\n",
    "    proj_root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Go up one directory\n",
    "    flashcards_folder = os.path.join(proj_root_path, 'flashcards')\n",
    "    images_folder = os.path.join(flashcards_folder, 'flashcard_images')\n",
    "    os.makedirs(flashcards_folder, exist_ok=True)\n",
    "    os.makedirs(images_folder, exist_ok=True)\n",
    "\n",
    "    # Create a safe filename for the flashcard and image\n",
    "    safe_name = front[:30].replace(' ', '_').replace('/', '_')\n",
    "    md_filename = safe_name + \".md\"\n",
    "    image_filename = safe_name + \".png\"  # save image as PNG\n",
    "    md_filepath = os.path.join(flashcards_folder, md_filename)\n",
    "    image_filepath = os.path.join(images_folder, image_filename)\n",
    "\n",
    "    # Generate and save the image\n",
    "    image = artist(front)\n",
    "\n",
    "    # Save the image\n",
    "    image.save(image_filepath)\n",
    "\n",
    "    # Relative path from md file to image (assuming md is in flashcards/, image in flashcard_images/)\n",
    "    relative_image_path = os.path.join('flashcard_images', image_filename)\n",
    "\n",
    "    # Write flashcard content with image placeholder\n",
    "    with open(md_filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(front + f\"\\n\\n![Image]({relative_image_path})\\n\\n\" + back)\n",
    "\n",
    "    print(f\"Flashcard saved to {md_filepath}\")\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"status\": \"success\",\n",
    "        \"flashcard_path\": md_filepath,\n",
    "        \"image_path\": image_filepath\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "76bc617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_call(message):\n",
    "    # Handle both message object format and dictionary format\n",
    "    if hasattr(message, 'tool_calls'):\n",
    "        # Original format: message object with tool_calls attribute\n",
    "        tool_call = message.tool_calls[0]\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        tool_call_id = tool_call.id\n",
    "    else:\n",
    "        # Dictionary format: message is a dict with function info\n",
    "        tool_call = message\n",
    "        arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "        tool_call_id = tool_call.get(\"id\")\n",
    "\n",
    "    # Here the tool is create_flashcard, expects JSON content as string\n",
    "    json_content = json.dumps(arguments)  # The arguments dict serialized to string\n",
    "\n",
    "    # Call your tool function\n",
    "    tool_response_str = create_flashcard(json_content)  # This returns a JSON string\n",
    "\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": tool_response_str,\n",
    "        \"tool_call_id\": tool_call_id\n",
    "    }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7b013783",
   "metadata": {},
   "outputs": [],
   "source": [
    "flashcard_creator = {\n",
    "            \"name\": \"create_flashcard\",\n",
    "            \"description\": \"Creates a flashcard with a title and explanation.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"front\": {\"type\": \"string\", \"description\": \"The flashcard title or question\"},\n",
    "                    \"back\": {\"type\": \"string\", \"description\": \"The flashcard answer or explanation\"}\n",
    "                },\n",
    "                \"required\": [\"front\", \"back\"]\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6b07ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": flashcard_creator}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5c7866da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def chat(history):\n",
    "    \"\"\"\n",
    "    Streaming chat that:\n",
    "    1) streams assistant text tokens as they arrive;\n",
    "    2) collects tool-call deltas into a full tool_calls payload;\n",
    "    3) executes tools when finish_reason == 'tool_calls';\n",
    "    4) continues the conversation and yields the final assistant turn (and image path if any).\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "\n",
    "    # First request: allow tools & stream tokens\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    reply = \"\"\n",
    "    image = None\n",
    "\n",
    "    # Buffer for tool calls: index -> dict(id, type=\"function\", function={name, arguments})\n",
    "    tool_calls_buffer = {}      # e.g., {0: {\"id\": \"...\", \"type\":\"function\", \"function\":{\"name\":\"...\", \"arguments\":\"{...}\"}}}\n",
    "    finish_reason = None\n",
    "\n",
    "    for chunk in stream:\n",
    "        choice = chunk.choices[0]\n",
    "        finish_reason = choice.finish_reason\n",
    "\n",
    "        # --- stream normal text tokens ---\n",
    "        delta = getattr(choice, \"delta\", None)\n",
    "        if delta is not None:\n",
    "            # A) text content\n",
    "            content_piece = getattr(delta, \"content\", None)\n",
    "            if content_piece:\n",
    "                reply += content_piece\n",
    "                # stream partial message to Chatbot\n",
    "                yield history + [{\"role\": \"assistant\", \"content\": reply}], image\n",
    "\n",
    "            # B) tool call deltas (support both shapes seen in client versions)\n",
    "            #    shape 1: delta.tool_calls -> list of deltas with .index, .id, .function\n",
    "            tool_calls_list = getattr(delta, \"tool_calls\", None)\n",
    "            if tool_calls_list:\n",
    "                for tc in tool_calls_list:\n",
    "                    idx = getattr(tc, \"index\", 0)\n",
    "                    buf = tool_calls_buffer.setdefault(idx, {\"id\": None, \"type\": \"function\", \"function\": {\"name\": None, \"arguments\": \"\"}})\n",
    "                    # id\n",
    "                    if getattr(tc, \"id\", None):\n",
    "                        buf[\"id\"] = tc.id\n",
    "                    # function name\n",
    "                    if getattr(tc.function, \"name\", None):\n",
    "                        buf[\"function\"][\"name\"] = tc.function.name\n",
    "                    # function arguments arrive as chunks of a JSON string\n",
    "                    if getattr(tc.function, \"arguments\", None):\n",
    "                        buf[\"function\"][\"arguments\"] += tc.function.arguments\n",
    "\n",
    "            #    shape 2: some client builds surface a single tool_call delta with .function\n",
    "            single_tc = getattr(delta, \"tool_call\", None)\n",
    "            if single_tc:\n",
    "                idx = getattr(single_tc, \"index\", 0)\n",
    "                buf = tool_calls_buffer.setdefault(idx, {\"id\": None, \"type\": \"function\", \"function\": {\"name\": None, \"arguments\": \"\"}})\n",
    "                if getattr(single_tc, \"id\", None):\n",
    "                    buf[\"id\"] = single_tc.id\n",
    "                if getattr(single_tc.function, \"name\", None):\n",
    "                    buf[\"function\"][\"name\"] = single_tc.function.name\n",
    "                if getattr(single_tc.function, \"arguments\", None):\n",
    "                    buf[\"function\"][\"arguments\"] += single_tc.function.arguments\n",
    "\n",
    "        # If the assistant decides it's done without tools, we'll see finish_reason == \"stop\"\n",
    "        if finish_reason == \"stop\":\n",
    "            # finalize this turn as plain text\n",
    "            history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            yield history, image\n",
    "            return\n",
    "\n",
    "        # If the assistant wants to call tools, we’ll see \"tool_calls\" once arguments are complete.\n",
    "        if finish_reason == \"tool_calls\":\n",
    "            break\n",
    "\n",
    "    # --- If tool calls were requested, execute them now ---\n",
    "    if tool_calls_buffer:\n",
    "        # Assemble assistant tool-call message (convert buffer to list in index order)\n",
    "        tool_calls = [tool_calls_buffer[i] for i in sorted(tool_calls_buffer.keys())]\n",
    "        messages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\n",
    "\n",
    "        # Execute each tool and append its result\n",
    "        for tc in tool_calls:\n",
    "            # tc[\"function\"][\"arguments\"] is a JSON string; parse safely\n",
    "            try:\n",
    "                args = json.loads(tc[\"function\"][\"arguments\"] or \"{}\")\n",
    "            except json.JSONDecodeError:\n",
    "                # try to repair common trailing JSON issues\n",
    "                fixed = (tc[\"function\"][\"arguments\"] or \"\").rstrip(\", \\n\")\n",
    "                try:\n",
    "                    args = json.loads(fixed)\n",
    "                except Exception:\n",
    "                    args = {}\n",
    "\n",
    "            # Your handler should accept a dict with keys like id, function{name, arguments(str)}\n",
    "            tool_response = handle_tool_call({\n",
    "                \"id\": tc.get(\"id\"),\n",
    "                \"function\": {\n",
    "                    \"name\": tc[\"function\"].get(\"name\"),\n",
    "                    \"arguments\": json.dumps(args)  # pass as string if your handler expects string\n",
    "                }\n",
    "            })\n",
    "\n",
    "            # Optionally surface an image immediately if your tool created one\n",
    "            try:\n",
    "                content_dict = json.loads(tool_response.get(\"content\", \"{}\"))\n",
    "                # support either key\n",
    "                img_path = content_dict.get(\"image_path\") or content_dict.get(\"flashcard_path\")\n",
    "                if img_path:\n",
    "                    image = img_path\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Append the tool result message (must be {\"role\":\"tool\",\"tool_call_id\":..., \"content\":...})\n",
    "            messages.append(tool_response)\n",
    "\n",
    "        # Follow-up assistant response after tools\n",
    "        followup = openai.chat.completions.create(model=MODEL_GPT, messages=messages)\n",
    "        reply = followup.choices[0].message.content\n",
    "\n",
    "        history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        yield history, image\n",
    "        return\n",
    "\n",
    "    # --- Fallback: if we got here without stop/tool_calls, just finalize the streamed text\n",
    "    history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    yield history, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4b678c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7909\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7909/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flashcard saved to /Users/severynkurach/Desktop/Programming/llm_engineering/flashcards/What_is_self-supervised_learni.md\n"
     ]
    }
   ],
   "source": [
    "# # More involved Gradio code as we're not using the preset Chat interface!\n",
    "# # Passing in inbrowser=True in the last line will cause a Gradio window to pop up immediately.\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "        image_output = gr.Image(height=500)\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def do_entry(message, history):\n",
    "        history += [{\"role\": \"user\", \"content\": message}]\n",
    "        return \"\", history\n",
    "\n",
    "    entry.submit(\n",
    "        do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]\n",
    "    ).then(\n",
    "        chat,      # generator, so Gradio streams automatically\n",
    "        inputs=chatbot,\n",
    "        outputs=[chatbot, image_output]\n",
    "    )\n",
    "\n",
    "    clear.click(lambda: [], inputs=None, outputs=chatbot, queue=False)\n",
    "\n",
    "ui.launch(allowed_paths=[\"/Users/severynkurach/Desktop/Programming/llm_engineering/flashcards\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
