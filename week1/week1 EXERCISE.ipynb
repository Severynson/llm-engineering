{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ollama\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key_OpenAI = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a personal tutor in LLM engineering, AI and Machine Learning. \n",
    "Respond the questions not only to give a dirrect solution, \n",
    "but explain the concept about which the question is asked, \n",
    "or give the framework for solution instead of answering only \n",
    "in order to help your student master their AI engineering skills.\n",
    "Priovide your response in markdown.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Name 3 main types of quantization for LLM models \n",
    "and explain how does they differ?\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8687dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(stream, version = 'OpenAI'):\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += (\n",
    "            getattr(chunk.message, 'content', '') if hasattr(chunk, 'message')\n",
    "            else getattr(chunk.choices[0].delta, 'content', '')\n",
    "        )\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Quantization is a crucial technique in optimizing large language models (LLMs) for efficiency in terms of computational resources and memory usage. By reducing the precision of the numbers used in model weights and activations, quantization enables faster computation and smaller memory footprints without significantly degrading model performance. Here are the three main types of quantization for LLM models:\n",
       "\n",
       "### 1. **Post-Training Quantization (PTQ)**\n",
       "\n",
       "**Concept**: \n",
       "Post-training quantization is applied to an already trained model. It doesn’t require retraining the model, making it a quick and simple method. PTQ typically helps in converting the weights of the floating-point representation into lower-bit formats (like 8-bit integers).\n",
       "\n",
       "**How It Works**:\n",
       "- The weights and activations of the model are quantized based on a predefined scale and zero-point.\n",
       "- Techniques such as **Min-Max Quantization** or **Mean-Std Quantization** are commonly used to determine the scaling factors.\n",
       "  \n",
       "**Advantages**:\n",
       "- Saves time since it doesn’t require retraining.\n",
       "- Good enough for many applications where extreme precision is not critical.\n",
       "\n",
       "**Disadvantages**:\n",
       "- Potential for loss in accuracy if the model is sensitive to the reduced precision of weights and activations.\n",
       "\n",
       "### 2. **Quantization-Aware Training (QAT)**\n",
       "\n",
       "**Concept**: \n",
       "Quantization-aware training incorporates quantization noise simulation in the training process itself. This allows the model to learn to adjust to the quantization effects during training, often leading to better performance in the quantized model.\n",
       "\n",
       "**How It Works**:\n",
       "- The model is trained with adjustments to weight and activation computations to account for quantization.\n",
       "- During training, both full-precision and quantized computations are used, allowing the model to learn robust representations.\n",
       "\n",
       "**Advantages**:\n",
       "- Generally results in higher accuracy compared to PTQ because the model learns to adapt to the quantization effects.\n",
       "- Prepares the model specifically for deployment in a quantized form.\n",
       "\n",
       "**Disadvantages**:\n",
       "- Longer training times since it may require additional training epochs.\n",
       "- More complex implementation compared to PTQ.\n",
       "\n",
       "### 3. **Dynamic Quantization**\n",
       "\n",
       "**Concept**: \n",
       "Dynamic quantization involves converting weights to lower precision during inference, without altering the original model during training. This method applies quantization on-the-fly for activation.\n",
       "\n",
       "**How It Works**:\n",
       "- The model weights are usually quantized to a lower bit-width before inference, but the activations are quantized dynamically as they are processed.\n",
       "  \n",
       "**Advantages**:\n",
       "- Can be applied easily with little overhead.\n",
       "- Provides a good balance between model performance and computational efficiency.\n",
       "\n",
       "**Disadvantages**:\n",
       "- May still consume considerable resources for certain operations and doesn't fully optimize the memory usage of model activations.\n",
       "\n",
       "### Summary of Differences\n",
       "\n",
       "| Feature                   | Post-Training Quantization | Quantization-Aware Training | Dynamic Quantization |\n",
       "|---------------------------|----------------------------|-----------------------------|----------------------|\n",
       "| Training Requirement       | No retraining              | Requires retraining         | No retraining        |\n",
       "| Accuracy Level             | Moderate                   | High                        | Moderate             |\n",
       "| Implementation Complexity   | Low                        | High                        | Moderate             |\n",
       "| Inference Speed            | Fast                       | Varies                      | Fast                 |\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Each type of quantization has its strengths and weaknesses, and the choice of which one to use can depend on the specific application, performance requirements, and acceptable trade-offs in model accuracy. As a student in AI engineering, you should deepen your understanding of these concepts and consider practical implementation examples to solidify your learning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "openai = OpenAI(api_key=api_key_OpenAI)\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "stream_response(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Quantization in Large Language Models (LLMs)**\n",
       "=====================================================\n",
       "\n",
       "Quantization is the process of reducing the precision of model weights and activations to improve inference speed and memory efficiency. In the context of Large Language Models (LLMs), quantization is crucial for achieving significant reductions in computational resources and energy consumption.\n",
       "\n",
       "### 1. **Integer Quantization**\n",
       "\n",
       "In integer quantization, both model weights and activations are represented using integers (typically 8-bit or 16-bit). This type of quantization provides a trade-off between precision and speed.\n",
       "\n",
       "**Pros:**\n",
       "\n",
       "* Fast inference speeds due to reduced floating-point operations\n",
       "* Reduced memory requirements\n",
       "\n",
       "**Cons:**\n",
       "\n",
       "* Lower accuracy compared to float32 (single-precision) models\n",
       "* Can suffer from loss of information, particularly for models with high computational complexity\n",
       "\n",
       "### 2. **Binary Quantization**\n",
       "\n",
       "In binary quantization, model weights and activations are represented using binary values (typically ±1). This type of quantization is more aggressive than integer quantization but still provides significant speed improvements.\n",
       "\n",
       "**Pros:**\n",
       "\n",
       "* Even faster inference speeds compared to integer quantization\n",
       "* Reduced memory requirements\n",
       "\n",
       "**Cons:**\n",
       "\n",
       "* Lower accuracy due to reduced representational capacity\n",
       "* Can suffer from non-deterministic behavior, particularly when dealing with stochastic activation functions like ReLU\n",
       "\n",
       "### 3. **Mixed Precision Quantization**\n",
       "\n",
       "In mixed precision quantization, model weights are represented using a combination of integer and binary values, while activations are represented using floating-point numbers (typically float32). This type of quantization aims to balance accuracy and speed.\n",
       "\n",
       "**Pros:**\n",
       "\n",
       "* Offers a trade-off between precision and speed\n",
       "* Can achieve better accuracy compared to pure integer or binary quantization\n",
       "\n",
       "**Cons:**\n",
       "\n",
       "* Requires more complex model design and optimization\n",
       "* May require additional computational resources for activation computations\n",
       "\n",
       "**Comparison of Quantization Types**\n",
       "------------------------------------\n",
       "\n",
       "| Quantization Type | Model Weights | Activations | Pros | Cons |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| Integer | Int8/Int16 | Float32 | Fast inference, reduced memory | Lower accuracy |\n",
       "| Binary | Int1 | ±1 | Fastest inference, reduced memory | Lowest accuracy |\n",
       "| Mixed Precision | Int8/Int16/Binary | Float32 | Balances precision and speed | Requires complex model design |\n",
       "\n",
       "In summary, the choice of quantization type depends on the specific use case and trade-off between accuracy and speed. Integer and binary quantization are more aggressive but provide faster inference speeds, while mixed precision quantization offers a balance between precision and speed.\n",
       "\n",
       "**Implementation Considerations**\n",
       "---------------------------------\n",
       "\n",
       "When implementing quantization in an LLM model, consider the following:\n",
       "\n",
       "* Use existing frameworks (e.g., TensorFlow Lite, PyTorch Quantization) to simplify the process\n",
       "* Choose an appropriate quantization scheme based on the specific use case and performance requirements\n",
       "* Monitor accuracy and adjust the quantization parameters as needed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "stream = ollama.chat(\n",
    "                    model=MODEL_LLAMA,\n",
    "                    messages=messages,\n",
    "                    stream=True\n",
    "                    )\n",
    "\n",
    "stream_response(stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
